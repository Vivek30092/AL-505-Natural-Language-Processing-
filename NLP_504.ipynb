{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ABm3h4INb2UJ"
      },
      "source": [
        "**Lab-1**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "opQrynppWnjw"
      },
      "outputs": [],
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7FGMw4TecDDE"
      },
      "source": [
        "**Lab-2**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ivXTUuBJkDxV",
        "outputId": "fd14102a-3001-49fb-a5cc-6ba929c55036"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Error loading WordNetLemmatizer: Package\n",
            "[nltk_data]     'WordNetLemmatizer' not found in index\n",
            "[nltk_data] Error loading PorterStemmer: Package 'PorterStemmer' not\n",
            "[nltk_data]     found in index\n",
            "[nltk_data] Error loading WordListCorpusReader: Package\n",
            "[nltk_data]     'WordListCorpusReader' not found in index\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('WordNetLemmatizer')\n",
        "nltk.download('PorterStemmer')\n",
        "nltk.download('WordListCorpusReader')0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xg9kCJUiXjbt",
        "outputId": "9326e245-8240-4009-c56f-3c55453d895d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[('apple', 1), ('looking', 1), ('buying', 1), ('startup', 1), ('billion', 1)]\n"
          ]
        }
      ],
      "source": [
        "from collections import Counter\n",
        "text='Apple is looking at buying a U.K. startup for $1 billion in London.'\n",
        "stopwords = set(stopwords.words('english'))\n",
        "tokens = word_tokenize(text.lower())\n",
        "filtered = [w for w in tokens if w.isalpha() and w not in stopwords]\n",
        "freq = Counter(filtered)\n",
        "print(freq.most_common(5))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Ou_YdOnEI8P",
        "outputId": "5bb41e9d-73fb-4c91-8f74-ef48b54a3041"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['progammer', 'program', 'programs', 'progamming']\n",
            "['progamm', 'program', 'program', 'progam']\n"
          ]
        }
      ],
      "source": [
        "# for stemming\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "porter_stemmer=PorterStemmer()\n",
        "words=['progammer', 'program','programs','progamming']\n",
        "stemming_words=[porter_stemmer.stem(word) for word in words]\n",
        "print(words)\n",
        "print(stemming_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "xLfKwCJkHflc"
      },
      "outputs": [],
      "source": [
        "d1='Today is Thursday.'\n",
        "d2='I am in Indore'\n",
        "d3='Potato is a mutual friend of every vegitable'\n",
        "\n",
        "list_of_token=word_tokenize(d1+d2+d3)\n",
        "stemming_words=[porter_stemmer.stem(word) for word in list_of_token]\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ocQgLgMVj5--"
      },
      "source": [
        "**C**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b2irnTV7K_IQ",
        "outputId": "8b28696c-90d5-4b87-940f-1568a48fb148"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vocabulary: {'live': 2, 'in': 0, 'indore': 1}\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "text = ['I live in Indore']\n",
        "vectorizer = CountVectorizer()\n",
        "x = vectorizer.fit_transform(text)\n",
        "\n",
        "\n",
        "print(\"Vocabulary:\", vectorizer.vocabulary_)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q7fdwcqEPTLj",
        "outputId": "4bf8d23e-e7e2-413c-af58-938a116c12e7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GTLYpfjljx7k"
      },
      "source": [
        "**Lemmatizer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k1JnpxqUMtx_",
        "outputId": "a15b70fa-b7a2-4b01-a3c7-a66dad90f051"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original Tokens: ['i', 'live', 'in', 'indore', 'potato', 'is', 'a', 'mutual', 'friend', 'of', 'every', 'vegitable']\n",
            "['i', 'live', 'in', 'indore', 'potato', 'is', 'a', 'mutual', 'friend', 'of', 'every', 'vegitable']\n"
          ]
        }
      ],
      "source": [
        "sentence='i live in Indore Potato is a mutual friend of every vegitable'\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "tokens = word_tokenize(sentence.lower())\n",
        "\n",
        "\n",
        "lemmas = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "\n",
        "print(\"Original Tokens:\", tokens)\n",
        "print(lemmas)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kPUmAh_vmZur"
      },
      "source": [
        "**Tf-Idf Vector**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "LySoxF4VmlrN"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from joblib import dump, load\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Ml9FZ_ldmvpY"
      },
      "outputs": [],
      "source": [
        "documents = [\n",
        "    \"This is the first document.\",\n",
        "    \"This document is the second document.\",\n",
        "    \"And this is the third one.\",\n",
        "    \"Is this the first document?\"\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "mwRxFcV9m2MJ"
      },
      "outputs": [],
      "source": [
        "vectorizer = TfidfVectorizer()\n",
        "X = vectorizer.fit_transform(documents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "XECco2hem7mB"
      },
      "outputs": [],
      "source": [
        "with open('tfidf_vectorizer.pkl', 'wb') as file:\n",
        "    pickle.dump(vectorizer, file)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "kt0WGp2jnDb-"
      },
      "outputs": [],
      "source": [
        "with open('tfidf_vectorizer.pkl', 'rb') as file:\n",
        "    loaded_vectorizer_pickle = pickle.load(file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rbOc83ClnFlO",
        "outputId": "3fc9ca45-da6b-4145-dcbf-9b0688ab60cb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['tfidf_vectorizer.joblib']"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dump(vectorizer, 'tfidf_vectorizer.joblib')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "KT2yy9bGnHsw"
      },
      "outputs": [],
      "source": [
        "loaded_vectorizer_joblib = load('tfidf_vectorizer.joblib')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "HiPLNlwBnK-e"
      },
      "outputs": [],
      "source": [
        "new_documents = [\n",
        "    \"This is a new document.\",\n",
        "    \"This document is different from the others.\"\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "4z5KyhrcnNxl"
      },
      "outputs": [],
      "source": [
        "X_new_pickle = loaded_vectorizer_pickle.transform(new_documents)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "yyjfU5sOnP27"
      },
      "outputs": [],
      "source": [
        "X_new_joblib = loaded_vectorizer_joblib.transform(new_documents)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Ev9XM0bnUqF",
        "outputId": "a11ab59b-fa12-4294-cf57-c1e943747173"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Feature names:\n",
            "['and' 'document' 'first' 'is' 'one' 'second' 'the' 'third' 'this']\n",
            "\n",
            "Original transformed data:\n",
            "[[0.         0.46979139 0.58028582 0.38408524 0.         0.\n",
            "  0.38408524 0.         0.38408524]\n",
            " [0.         0.6876236  0.         0.28108867 0.         0.53864762\n",
            "  0.28108867 0.         0.28108867]\n",
            " [0.51184851 0.         0.         0.26710379 0.51184851 0.\n",
            "  0.26710379 0.51184851 0.26710379]\n",
            " [0.         0.46979139 0.58028582 0.38408524 0.         0.\n",
            "  0.38408524 0.         0.38408524]]\n",
            "\n",
            "Transformed new data using loaded vectorizer from pickle:\n",
            "[[0.         0.65416415 0.         0.53482206 0.         0.\n",
            "  0.         0.         0.53482206]\n",
            " [0.         0.57684669 0.         0.47160997 0.         0.\n",
            "  0.47160997 0.         0.47160997]]\n",
            "\n",
            "Transformed new data using loaded vectorizer from joblib:\n",
            "[[0.         0.65416415 0.         0.53482206 0.         0.\n",
            "  0.         0.         0.53482206]\n",
            " [0.         0.57684669 0.         0.47160997 0.         0.\n",
            "  0.47160997 0.         0.47160997]]\n"
          ]
        }
      ],
      "source": [
        "print(\"Feature names:\")\n",
        "print(vectorizer.get_feature_names_out())\n",
        "\n",
        "print(\"\\nOriginal transformed data:\")\n",
        "print(X.toarray())\n",
        "\n",
        "print(\"\\nTransformed new data using loaded vectorizer from pickle:\")\n",
        "print(X_new_pickle.toarray())\n",
        "\n",
        "print(\"\\nTransformed new data using loaded vectorizer from joblib:\")\n",
        "print(X_new_joblib.toarray())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9pN_SkyKnXr7",
        "outputId": "15b9850d-89b6-41bf-bdb7-ff07eca778db"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unigrams: [('this',), ('is',), ('a',), ('sample',), ('sentence',), ('to',), ('demonstrate',), ('n-grams',), ('in',), ('nlp',), ('.',)]\n",
            "Bigrams: [('this', 'is'), ('is', 'a'), ('a', 'sample'), ('sample', 'sentence'), ('sentence', 'to'), ('to', 'demonstrate'), ('demonstrate', 'n-grams'), ('n-grams', 'in'), ('in', 'nlp'), ('nlp', '.')]\n",
            "Trigrams: [('this', 'is', 'a'), ('is', 'a', 'sample'), ('a', 'sample', 'sentence'), ('sample', 'sentence', 'to'), ('sentence', 'to', 'demonstrate'), ('to', 'demonstrate', 'n-grams'), ('demonstrate', 'n-grams', 'in'), ('n-grams', 'in', 'nlp'), ('in', 'nlp', '.')]\n",
            "\n",
            "Bigram Counts: [(('this', 'is'), 1), (('is', 'a'), 1), (('a', 'sample'), 1), (('sample', 'sentence'), 1), (('sentence', 'to'), 1)]\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.util import ngrams\n",
        "from collections import Counter\n",
        "\n",
        "text = \"This is a sample sentence to demonstrate N-grams in NLP.\"\n",
        "tokens = nltk.word_tokenize(text.lower())\n",
        "\n",
        "unigrams = list(ngrams(tokens, 1))\n",
        "print(\"Unigrams:\", unigrams)\n",
        "\n",
        "bigrams = list(ngrams(tokens, 2))\n",
        "print(\"Bigrams:\", bigrams)\n",
        "\n",
        "trigrams = list(ngrams(tokens, 3))\n",
        "print(\"Trigrams:\", trigrams)\n",
        "\n",
        "bigram_counts = Counter(bigrams)\n",
        "print(\"\\nBigram Counts:\", bigram_counts.most_common(5))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ul9MhH5Yr-gS",
        "outputId": "b43eb979-3d8f-4c64-f86f-70a683a86b15"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vocabulary: ['and' 'another' 'example' 'fun' 'is' 'nlp' 'powerful' 'sample' 'sentence'\n",
            " 'this']\n",
            "BoW Matrix:\n",
            " [[0 0 0 0 1 0 0 1 1 1]\n",
            " [0 1 1 0 1 0 0 0 1 1]\n",
            " [1 0 0 1 1 1 1 0 0 0]]\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Sample corpus\n",
        "documents = [\n",
        "    \"This is a sample sentence\",\n",
        "    \"This sentence is another example\",\n",
        "    \"NLP is fun and powerful\"\n",
        "]\n",
        "\n",
        "# Create the Bag of Words model\n",
        "vectorizer = CountVectorizer()\n",
        "X = vectorizer.fit_transform(documents)\n",
        "\n",
        "# Feature names (unique words)\n",
        "print(\"Vocabulary:\", vectorizer.get_feature_names_out())\n",
        "\n",
        "# Bag of Words matrix\n",
        "print(\"BoW Matrix:\\n\", X.toarray())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lQa6-sY4r-PY"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kyj8wLtpsrzc",
        "outputId": "70be83cb-bfbf-4c89-deed-33e31ef73311"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vocabulary: ['fun' 'is' 'nlp']\n",
            "One Hot Encoding:\n",
            " [[0 0 1]\n",
            " [0 1 0]\n",
            " [1 0 0]]\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "\n",
        "# Sample text\n",
        "sentence = \"NLP is fun\"\n",
        "tokens = nltk.word_tokenize(sentence.lower())\n",
        "\n",
        "# One-hot encoding\n",
        "encoder = LabelBinarizer()\n",
        "one_hot = encoder.fit_transform(tokens)\n",
        "\n",
        "print(\"Vocabulary:\", encoder.classes_)\n",
        "print(\"One Hot Encoding:\\n\", one_hot)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PLpHk0U3ssxK"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
