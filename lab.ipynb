{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f816f6b",
   "metadata": {},
   "source": [
    "> # stemming "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5aa98292",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original words : ['programming', 'program', 'programs', 'program', 'programed']\n",
      "stemmed words : ['program', 'program', 'program', 'program', 'program']\n"
     ]
    }
   ],
   "source": [
    "# for stemming\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "poeter_stemmer = PorterStemmer()\n",
    "words = [\"programming\",\"program\",\"programs\",\"program\", \"programed\"]\n",
    "\n",
    "stemmed_words = [poeter_stemmer.stem(word) for word in words]\n",
    "\n",
    "print(f\"original words : {words}\")\n",
    "print(f\"stemmed words : {stemmed_words}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a25515",
   "metadata": {},
   "source": [
    "> # Task1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "895c2caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = \"Today is Thursday\"\n",
    "s2 = \"I am in Indore\"\n",
    "s3 = \"Potato is a mutual freind of every other vegetable\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "646cec6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "list_ofTokens =  word_tokenize(s1+s2+s3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1a1e75ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original words : ['Today', 'is', 'ThursdayI', 'am', 'in', 'IndorePotato', 'is', 'a', 'mutual', 'freind', 'of', 'every', 'other', 'vegetable']\n",
      "stemmed words : ['today', 'is', 'thursdayi', 'am', 'in', 'indorepotato', 'is', 'a', 'mutual', 'freind', 'of', 'everi', 'other', 'veget']\n"
     ]
    }
   ],
   "source": [
    "stemmed_words1 = [poeter_stemmer.stem(word) for word in list_ofTokens]\n",
    "\n",
    "print(f\"original words : {list_ofTokens}\")\n",
    "print(f\"stemmed words : {stemmed_words1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a801568b",
   "metadata": {},
   "source": [
    "> # Count Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f8a0a04e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary:  {'love': 1, 'nlp': 2, 'python': 3, 'ai': 0}\n"
     ]
    }
   ],
   "source": [
    "#  # for counter vector\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "doc = [\"i love NLP\",\"i love python\",\"i love AI\"]\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Create a CountVectorizer object\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Fit and transform the documents to create a document-term matrix\n",
    "x = vectorizer.fit_transform(doc)\n",
    "\n",
    "# Print the vocabulary\n",
    "print(\"Vocabulary: \", vectorizer.vocabulary_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3bcdad1",
   "metadata": {},
   "source": [
    "> # lemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ffea4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\sait\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import nltk\n",
    "# nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "08ef37d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text: I am a student in a college \n",
      "Lemmatized Words: ['I', 'am', 'a', 'student', 'in', 'a', 'college']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "text = \"I am a student in a college \"\n",
    "\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "lemmatized_words = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "\n",
    "print(f\"Original Text: {text}\")\n",
    "print(f\"Lemmatized Words: {lemmatized_words}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "20c35b95",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\sait\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data] Error loading won_1.4: Package 'won_1.4' not found in\n",
      "[nltk_data]     index\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('won_1.4')\n",
    "words = [\"running\",\"better\",\"studio\",\"mice\"]\n",
    "\n",
    "for word in words:\n",
    "    lemma = lemmatizer.lemmatize(word)\n",
    "    print(f\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f101b228",
   "metadata": {},
   "source": [
    "> # TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7acc58ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature names: ['document' 'doucmentthis' 'first' 'is' 'one' 'secondand' 'the' 'third'\n",
      " 'this' 'thsi']\n",
      "TF-IDF Matix: \n",
      " <Compressed Sparse Row sparse matrix of dtype 'float64'\n",
      "\twith 10 stored elements and shape (1, 10)>\n",
      "  Coords\tValues\n",
      "  (0, 8)\t0.23570226039551587\n",
      "  (0, 3)\t0.7071067811865476\n",
      "  (0, 6)\t0.23570226039551587\n",
      "  (0, 2)\t0.23570226039551587\n",
      "  (0, 1)\t0.23570226039551587\n",
      "  (0, 0)\t0.23570226039551587\n",
      "  (0, 5)\t0.23570226039551587\n",
      "  (0, 9)\t0.23570226039551587\n",
      "  (0, 7)\t0.23570226039551587\n",
      "  (0, 4)\t0.23570226039551587\n",
      "Dense TF.IDF Matrix :\n",
      " [[0.23570226 0.23570226 0.23570226 0.70710678 0.23570226 0.23570226\n",
      "  0.23570226 0.23570226 0.23570226 0.23570226]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import  TfidfVectorizer\n",
    "corpus = [\n",
    "    \"This is the first doucment\"\n",
    "    \"This document is second\"\n",
    "    \"and thsi is third one\"\n",
    "]\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(corpus)\n",
    "print(\"Feature names:\" ,vectorizer.get_feature_names_out())\n",
    "\n",
    "print(\"TF-IDF Matix: \\n\",tfidf_matrix)\n",
    "print(\"Dense TF.IDF Matrix :\\n\",tfidf_matrix.toarray())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba9761d",
   "metadata": {},
   "source": [
    "> # N-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "53f053b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unigrams: [('this',), ('is',), ('a',), ('sample',), ('sentence',), ('to',), ('demonstrate',), ('n-grams',), ('in',), ('nlp',), ('.',)]\n",
      "Bigrams: [('this', 'is'), ('is', 'a'), ('a', 'sample'), ('sample', 'sentence'), ('sentence', 'to'), ('to', 'demonstrate'), ('demonstrate', 'n-grams'), ('n-grams', 'in'), ('in', 'nlp'), ('nlp', '.')]\n",
      "Trigrams: [('this', 'is', 'a'), ('is', 'a', 'sample'), ('a', 'sample', 'sentence'), ('sample', 'sentence', 'to'), ('sentence', 'to', 'demonstrate'), ('to', 'demonstrate', 'n-grams'), ('demonstrate', 'n-grams', 'in'), ('n-grams', 'in', 'nlp'), ('in', 'nlp', '.')]\n",
      "\n",
      "Bigram Counts: [(('this', 'is'), 1), (('is', 'a'), 1), (('a', 'sample'), 1), (('sample', 'sentence'), 1), (('sentence', 'to'), 1)]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "from collections import Counter\n",
    "\n",
    "text = \"This is a sample sentence to demonstrate N-grams in NLP.\"\n",
    "tokens = nltk.word_tokenize(text.lower())\n",
    "\n",
    "unigrams = list(ngrams(tokens, 1))\n",
    "print(\"Unigrams:\", unigrams)\n",
    "\n",
    "bigrams = list(ngrams(tokens, 2))\n",
    "print(\"Bigrams:\", bigrams)\n",
    "\n",
    "trigrams = list(ngrams(tokens, 3))\n",
    "print(\"Trigrams:\", trigrams)\n",
    "\n",
    "bigram_counts = Counter(bigrams)\n",
    "print(\"\\nBigram Counts:\", bigram_counts.most_common(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76cb3da5",
   "metadata": {},
   "source": [
    "> # bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a329efea",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature_extraction\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CountVectorizer\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      4\u001b[0m documents \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthis quick  brown for jumps over the lazy dog\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe dog berks loudly\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma quick for runs fast\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      8\u001b[0m ]\n\u001b[0;32m      9\u001b[0m vectorizer \u001b[38;5;241m=\u001b[39m CountVectorizer()\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "documents = [\n",
    "    \"this quick  brown for jumps over the lazy dog\",\n",
    "    \"the dog berks loudly\",\n",
    "    \"a quick for runs fast\"\n",
    "]\n",
    "vectorizer = CountVectorizer()\n",
    "bow_matrix = vectorizer.fit_transform(documents)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "bow_df = pd.Dataframe(bow_matrix.toarry(),columns = feature_names)\n",
    "print(f\"bag of words matrix : {bow_df}\")\n",
    "print(f\"vocabulary(feature names): {features_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c567a69",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
